# Awesome-Minecraft-Agents

## Our Minecraft Agent

<h3><img src="./images/optimus.jpg" style="height: 2em; margin-right: 0.5em;">Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</h3>  
<p align="center">
    <img src="./images/framework.png" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[üçé Project Page](https://cybertronagent.github.io/Optimus-1.github.io/)] [[üìñ arXiv Paper](https://arxiv.org/abs/2408.03615)] [[üåü GitHub](https://github.com/JiuTian-VL/Optimus-1)] </div></font>

We propose a **Hybrid Multimodal Memory** module that integrates structured knowledge and multimodal experiences into the memory mechanism of the agent. On top of it, we introduce a powerful Minecraft agent, **Optimus-1**, which achieves a **30\%** improvement over existing agents on **67** long-horizon tasks. ‚ú®  

<h3><img src="./images/optimus2.png" style="height: 2em; margin-right: 0.5em;">Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy</h3>  
<p align="center">
    <img src="./images/framework2.png" width="80%" height="80%">
</p>

<font size=7><div align='center' >[[üçé Project Page](https://cybertronagent.github.io/Optimus-2.github.io/)] [[üìñ arXiv Paper](https://arxiv.org/abs/2502.19902)] [[üåü GitHub](https://github.com/lizaijing/Optimus-2)] </div></font>

We propose agent Optimus-2 which incorporates a Multimodal Large Language Model for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. **Optimus-2** exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft. ‚ú®

<h3><img src="./images/optimus3.png" style="height: 2em; margin-right: 0.5em;">Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts</h3>  
<p align="center">
    <img src="./images/framework3.png" width="80%" height="80%">
</p>

<font size=7><div align='center' >[[üçé Project Page](https://cybertronagent.github.io/Optimus-3.github.io/)] [[üìñ arXiv Paper](https://arxiv.org/abs/2506.10357)] [[üåü GitHub](https://github.com/lizaijing/Optimus-3)] </div></font>

We propose generalist agent, **Optimus-3**, endowed with multidimensional capabilities including Captioning, Embodied QA, Planning, Action, Grounding, and Reflection. Our comprehensive evaluation demonstrates that it consistently surpasses existing agents in the Minecraft environment across all assessed dimensions. ‚ú®



---
<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Policy](#awesome-policy)
  - [Vision-driven Policy](#vision-driven-policy)
  - [Goal-conditioned Policy](#goal-conditioned-policy)
  
- [Awesome Agent](#awesome-agents)
  - [Policy-based Agent](#policy-based-agent)
  - [Code-based Agent](#code-based-agent)

---

# Awesome Policy

## Vision-driven Policy
|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/openai/Video-Pre-Training.svg?style=social&label=Star) <br> [**Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos**](https://arxiv.org/abs/2206.11795) <br> | NeurIPS | 2022 | [Github](https://github.com/openai/Video-Pre-Training) | - | 
| ![Star](https://img.shields.io/github/stars/CraftJarvis/GROOT.svg?style=social&label=Star) <br> [**GROOT: Learning to Follow Instructions by Watching Gameplay Videos**](https://arxiv.org/abs/2310.08235) <br> | ICLR | 2024 | [Github](https://github.com/CraftJarvis/GROOT) | [Demo](https://craftjarvis.github.io/GROOT/)| 

## Goal-conditioned Policy
|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/MineDojo/MineDojo.svg?style=social&label=Star) <br> [**MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**](https://arxiv.org/abs/2206.08853) <br> | NeurIPS | 2022 | [Github](https://github.com/MineDojo/MineDojo) | [Demo](https://minedojo.org/) |
| ![Star](https://img.shields.io/github/stars/danijar/dreamerv3.svg?style=social&label=Star) <br> [**Mastering Diverse Domains through World Models**](https://arxiv.org/abs/2301.04104v2) <br> | Nature | 2025 | [Github](https://github.com/danijar/dreamerv3) | [Demo](https://danijar.com/project/dreamerv3/) |
| ![Star](https://img.shields.io/github/stars/Shalev-Lifshitz/STEVE-1.svg?style=social&label=Star) <br> [**STEVE-1: A Generative Model for Text-to-Behavior in Minecraft**](https://arxiv.org/abs/2306.00937) <br> | NeurIPS | 2023 | [Github](https://github.com/Shalev-Lifshitz/STEVE-1) | [Demo](https://sites.google.com/view/steve-1) |
| ![Star](https://img.shields.io/github/stars/CraftJarvis/MC-Controller.svg?style=social&label=Star) <br> [**Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction**](https://arxiv.org/abs/2301.10034) <br> | CVPR | 2023 | [Github](https://github.com/CraftJarvis/MC-Controller) | - |
| ![Star](https://img.shields.io/github/stars/Zhoues/MineDreamer.svg?style=social&label=Star) <br> [**MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control**](https://arxiv.org/abs/2403.12037) <br> | NeurIPS Workshop | 2024 | [Github](https://github.com/Zhoues/MineDreamer/) | [Demo](https://sites.google.com/view/minedreamer/main)| 
| ![Star](https://img.shields.io/github/stars/PKU-RL/PTGM.svg?style=social&label=Star) <br> [**Pre-Training Goal-Based Models for Sample-Efficient Reinforcement Learning**](https://openreview.net/forum?id=o2IEmeLL9r) <br> | ICLR | 2024 | [Github](https://github.com/PKU-RL/PTGM) | - | 
| [**Vision-Language Models Provide Promptable Representations for Reinforcement Learning**](https://arxiv.org/abs/2402.02651) <br> | Arxiv | 2024 | [Github](https://github.com/pr2l/pr2l.github.io/tree/master/static/notebooks) | - | 
| ![Star](https://img.shields.io/github/stars/PKU-RL/CLIP4MC.svg?style=social&label=Star) <br> [**Reinforcement Learning Friendly Vision-Language Model for MineCraft**](https://arxiv.org/abs/2303.10571) <br> | ECCV | 2024 | [Github](https://github.com/PKU-RL/CLIP4MC) | - | 
| ![Star](https://img.shields.io/github/stars/CraftJarvis/ROCKET-1.svg?style=social&label=Star) <br> [**ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting**](https://arxiv.org/abs/2410.17856) <br> | CVPR | 2025 | [Github](https://github.com/CraftJarvis/ROCKET-1) | [Demo](https://craftjarvis.github.io/ROCKET-1/) | 
|  [**GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents**](https://arxiv.org/abs/2412.10410) <br> | ICLR | 2025 | - | - | 
| [**Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy**](https://arxiv.org/abs/2502.19902) <br> | CVPR | 2025 | [Github](https://github.com/dawn0815/Optimus-2) | [Demo](https://cybertronagent.github.io/Optimus-2.github.io/) | 
| [**ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment**](https://arxiv.org/abs/2503.02505) <br> | Arxiv | 2025 | [Github](https://github.com/CraftJarvis/ROCKET-2) | [Demo](https://craftjarvis.github.io/ROCKET-2/) | 
| [**Open-World Skill Discovery from Unsegmented Demonstrations**](https://arxiv.org/abs/2503.10684) <br> | Arxiv | 2025 | [Github](https://github.com/CraftJarvis/SkillDiscovery) | [Demo](https://craftjarvis.github.io/SkillDiscovery/) | 

---

# Awesome Agent

## Policy-based Agent
|  Title  |   Venue  |   Year  |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/CraftJarvis/MC-Planner.svg?style=social&label=Star) <br> [**Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**](https://arxiv.org/abs/2302.01560) <br> | NeurIPS | 2023 | [Github](https://github.com/CraftJarvis/MC-Planner) | [Demo](https://craftjarvis.github.io/) |
| ![Star](https://img.shields.io/github/stars/PKU-RL/Plan4MC.svg?style=social&label=Star) <br> [**Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks**](https://arxiv.org/abs/2303.16563) <br> | NeurIPS Workshop | 2023 | [Github](https://github.com/PKU-RL/Plan4MC) | [Demo](https://sites.google.com/view/plan4mc) |
| ![Star](https://img.shields.io/github/stars/zhoubohan0/STG-Transformer.svg?style=social&label=Star) <br> [**Learning from Visual Observation via Offline Pretrained State-to-Go Transformer**](https://arxiv.org/abs/2306.12860) <br> | NeurIPS | 2023 | [Github](https://github.com/zhoubohan0/STG-Transformer) | [Demo](https://sites.google.com/view/stgtransformer) |
| [**LLaMA Rider: Spurring Large Language Models to Explore the Open World**](https://arxiv.org/abs/2310.08922) <br> | NAACL Findings | 2024 | - | - |
| ![Star](https://img.shields.io/github/stars/CraftJarvis/JARVIS-1.svg?style=social&label=Star) <br> [**JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models**](https://arxiv.org/abs/2311.05997) <br> | NeurIPS Workshop | 2023| [Github](https://github.com/CraftJarvis/JARVIS-1) | [Demo](https://craftjarvis-jarvis1.github.io/) |
| ![Star](https://img.shields.io/github/stars/BAAI-Agents/Steve-Eye.svg?style=social&label=Star) <br> [**Steve-Eye: Equiping LLM-based Embodied Agents with Visual Perception in Open Worlds**](https://arxiv.org/abs/2310.13255) <br> | ICLR | 2024| [Github](https://github.com/BAAI-Agents/Steve-Eye) | [Demo](https://sites.google.com/view/steve-eye) |
| ![Star](https://img.shields.io/github/stars/JiuTian-VL/Optimus-1.svg?style=social&label=Star) <br> [**Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**](https://arxiv.org/abs/2408.03615) <br> | NeurIPS | 2024 | [Github](https://github.com/JiuTian-VL/Optimus-1) | [Demo](https://cybertronagent.github.io/Optimus-1.github.io/) | 
| ![Star](https://img.shields.io/github/stars/IranQin/MP5.svg?style=social&label=Star) <br> [**MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**](https://arxiv.org/abs/2312.07472) <br> | CVPR | 2024 | [Github](https://github.com/IranQin/MP5) | [Demo](https://iranqin.github.io/MP5.github.io/) | 
| ![Star](https://img.shields.io/github/stars/CraftJarvis/OmniJarvis.svg?style=social&label=Star) <br> [**OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents**](https://arxiv.org/abs/2407.00114) <br> | NeurIPS | 2024 | [Github](https://github.com/CraftJarvis/OmniJarvis) | [Demo](https://omnijarvis.github.io/) | 
| [**MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory**](https://arxiv.org/abs/2411.06736) <br> | ICLR | 2025 | - | [Demo](https://sites.google.com/view/mr-steve) | 
| [**ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting**](https://arxiv.org/abs/2410.17856) <br> | CVPR | 2025 | [Github](https://github.com/CraftJarvis/ROCKET-1) | [Demo](https://craftjarvis.github.io/ROCKET-1/) | 
| [**Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy**](https://arxiv.org/abs/2502.19902) <br> | CVPR | 2025 | [Github](https://github.com/dawn0815/Optimus-2) | [Demo](https://cybertronagent.github.io/Optimus-2.github.io/) | 
| [**JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse**](https://arxiv.org/abs/2503.16365) <br> | Arxiv | 2025 | [Github](https://github.com/CraftJarvis/JarvisVLA) | [Demo](https://craftjarvis.github.io/JarvisVLA/) | 

## Code-based Agent
|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/MineDojo/Voyager.svg?style=social&label=Star) <br> [**Voyager: An Open-Ended Embodied Agent with Large Language Models**](https://arxiv.org/abs/2305.16291) <br> |  NeurIPS | 2023 | [Github](https://github.com/MineDojo/Voyager) | [Demo](https://voyager.minedojo.org/) |
| ![Star](https://img.shields.io/github/stars/OpenGVLab/GITM.svg?style=social&label=Star) <br> [**Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**](https://arxiv.org/abs/2305.17144) <br> |  Arxiv | 2023 | [Github](https://github.com/OpenGVLab/GITM) | - |
| ![Star](https://img.shields.io/github/stars/PKU-RL/Creative-Agents.svg?style=social&label=Star) <br> [**Creative Agents: Empowering Agents with Imagination for Creative Tasks**](https://arxiv.org/abs/2312.02519) <br> |  Arxiv | 2023 | [Github](https://github.com/PKU-RL/Creative-Agents) | [Demo](https://sites.google.com/view/creative-agents) |
|  [**Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**](https://arxiv.org/abs/2312.09238) <br> | CVPR | 2024 | - | [Demo](https://yangxue0827.github.io/auto_mc-reward.html) |
| ![Star](https://img.shields.io/github/stars/zju-vipa/Odyssey.svg?style=social&label=Star) <br> [**Odyssey: Empowering Minecraft Agents with Open-World Skills**](https://arxiv.org/abs/2407.15325) <br> |  Arxiv | 2024 | [Github](https://github.com/zju-vipa/Odyssey) | - |
| ![Star](https://img.shields.io/github/stars/rese1f/STEVE.svg?style=social&label=Star) <br> [**See and Think: Embodied Agent in Virtual Environment**](https://arxiv.org/abs/2311.15209) <br> |  ECCV | 2024 | [Github](https://github.com/rese1f/STEVE) | [Demo](https://rese1f.github.io/STEVE/) |
| [**RL-GPT: Integrating Reinforcement Learning and Code-as-policy**](https://arxiv.org/abs/2402.19299) <br> |  NeurIPS | 2024 | - | [Demo](https://sites.google.com/view/rl-gpt) |
| [**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**](https://arxiv.org/abs/2405.17424) <br> |  Arxiv | 2024 | - | [Demo](https://lizhuoling.github.io/LARM_webpage/) |
| [**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**](https://arxiv.org/abs/2405.15414) <br> |  Arxiv | 2024 | - | - |


