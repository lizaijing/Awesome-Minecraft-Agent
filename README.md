# Awesome-Minecraft-Agents

## Our Minecraft Agent

<img src="./images/optimus.jpg" style="vertical-align: middle; height: 1em; padding: 0 0.2em;"> **Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**  
<p align="center">
    <img src="./images/framework.png" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[üçé Project Page](https://cybertronagent.github.io/Optimus-1.github.io/)] [[üìñ arXiv Paper](https://arxiv.org/abs/2408.03615)] [[üåü GitHub](https://github.com/JiuTian-VL/Optimus-1)] </div></font>

The VITA team proposes Freeze-Omni, a speech-to-speech dialogue model with both low-latency and high intelligence while the training process is based on a frozen LLM. üåü 

Freeze-Omni exhibits the characteristic of being **smart** as it is constructed upon a **frozen** text-modality LLM. This enables it to keep the original intelligence of the LLM backbone, without being affected by the forgetting problem induced by the fine-tuning process for integration of the speech modality. ‚ú® 


---
<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Policy](#awesome-policy)
  - [Vision-driven Policy](#vision-driven-policy)
  - [Goal-conditioned Policy](#goal-conditioned-policy)
  
- [Awesome Agent](#awesome-agents)
  - [Policy-based Agent](#policy-based-agent)
  - [Code-based Agent](#code-based-agent)

---

# Awesome Policy

## Vision-driven Policy
|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/openai/Video-Pre-Training.svg?style=social&label=Star) <br> [**Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos**](https://arxiv.org/abs/2206.11795) <br> | NeurIPS | 2022 | [Github](https://github.com/openai/Video-Pre-Training) | - | 
| ![Star](https://img.shields.io/github/stars/CraftJarvis/GROOT.svg?style=social&label=Star) <br> [**GROOT: Learning to Follow Instructions by Watching Gameplay Videos**](https://arxiv.org/abs/2310.08235) <br> | ICLR | 2024 | [Github](https://github.com/CraftJarvis/GROOT) | [Demo](https://craftjarvis.github.io/GROOT/)| 

## Goal-conditioned Policy
|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/MineDojo/MineDojo.svg?style=social&label=Star) <br> [**MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**](https://arxiv.org/abs/2206.08853) <br> | NeurIPS | 2022 | [Github](https://github.com/MineDojo/MineDojo) | [Demo](https://minedojo.org/) |
| ![Star](https://img.shields.io/github/stars/danijar/dreamerv3.svg?style=social&label=Star) <br> [**Mastering Diverse Domains through World Models**](https://arxiv.org/abs/2301.04104v2) <br> | Arxiv | 2023 | [Github](https://github.com/danijar/dreamerv3) | [Demo](https://danijar.com/project/dreamerv3/) |
| ![Star](https://img.shields.io/github/stars/Shalev-Lifshitz/STEVE-1.svg?style=social&label=Star) <br> [**STEVE-1: A Generative Model for Text-to-Behavior in Minecraft**](https://arxiv.org/abs/2306.00937) <br> | NeurIPS | 2023 | [Github](https://github.com/Shalev-Lifshitz/STEVE-1) | [Demo](https://sites.google.com/view/steve-1) |
| ![Star](https://img.shields.io/github/stars/CraftJarvis/MC-Controller.svg?style=social&label=Star) <br> [**Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction**](https://arxiv.org/abs/2301.10034) <br> | CVPR | 2023 | [Github](https://github.com/CraftJarvis/MC-Controller) | - |
| ![Star](https://img.shields.io/github/stars/Zhoues/MineDreamer.svg?style=social&label=Star) <br> [**MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control**](https://arxiv.org/abs/2403.12037) <br> | NeurIPS Workshop | 2024 | [Github](https://github.com/Zhoues/MineDreamer/) | [Demo](https://sites.google.com/view/minedreamer/main)| 
| [**Vision-Language Models Provide Promptable Representations for Reinforcement Learning**](https://arxiv.org/abs/2402.02651) <br> | Arxiv | 2024 | [Github](https://github.com/pr2l/pr2l.github.io/tree/master/static/notebooks) | - | 
| ![Star](https://img.shields.io/github/stars/CraftJarvis/ROCKET-1.svg?style=social&label=Star) <br> [**ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting**](https://arxiv.org/abs/2410.17856) <br> | Arxiv | 2024 | [Github](https://github.com/CraftJarvis/ROCKET-1) | [Demo](https://craftjarvis.github.io/ROCKET-1/) | 

---

# Awesome Agent

## Policy-based Agent
|  Title  |   Venue  |   Year  |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/CraftJarvis/MC-Planner.svg?style=social&label=Star) <br> [**Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**](https://arxiv.org/abs/2302.01560) <br> | NeurIPS | 2023 | [Github](https://github.com/CraftJarvis/MC-Planner) | [Demo](https://craftjarvis.github.io/) |
| ![Star](https://img.shields.io/github/stars/PKU-RL/Plan4MC.svg?style=social&label=Star) <br> [**Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks**](https://arxiv.org/abs/2303.16563) <br> | NeurIPS Workshop | 2023 | [Github](https://github.com/PKU-RL/Plan4MC) | [Demo](https://sites.google.com/view/plan4mc) |
| [**LLaMA Rider: Spurring Large Language Models to Explore the Open World**](https://arxiv.org/abs/2310.08922) <br> | Arxiv | 2023 | - | - |
| ![Star](https://img.shields.io/github/stars/CraftJarvis/JARVIS-1.svg?style=social&label=Star) <br> [**JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models**](https://arxiv.org/abs/2311.05997) <br> | NeurIPS Workshop | 2023| [Github](https://github.com/CraftJarvis/JARVIS-1) | [Demo](https://craftjarvis-jarvis1.github.io/) |
| ![Star](https://img.shields.io/github/stars/JiuTian-VL/Optimus-1.svg?style=social&label=Star) <br> [**Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**](https://arxiv.org/abs/2408.03615) <br> | NeurIPS | 2024 | [Github](https://github.com/JiuTian-VL/Optimus-1) | [Demo](https://cybertronagent.github.io/Optimus-1.github.io/) | 
| ![Star](https://img.shields.io/github/stars/IranQin/MP5.svg?style=social&label=Star) <br> [**MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**](https://arxiv.org/abs/2312.07472) <br> | CVPR | 2024 | [Github](https://github.com/IranQin/MP5) | [Demo](https://iranqin.github.io/MP5.github.io/) | 
| ![Star](https://img.shields.io/github/stars/CraftJarvis/OmniJarvis.svg?style=social&label=Star) <br> [**OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents**](https://arxiv.org/abs/2407.00114) <br> | NeurIPS | 2024 | [Github](https://github.com/CraftJarvis/OmniJarvis) | [Demo](https://omnijarvis.github.io/) | 

## Code-based Agent
|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/MineDojo/Voyager.svg?style=social&label=Star) <br> [**Voyager: An Open-Ended Embodied Agent with Large Language Models**](https://arxiv.org/abs/2305.16291) <br> |  NeurIPS | 2023 | [Github](https://github.com/MineDojo/Voyager) | [Demo](https://voyager.minedojo.org/) |
| ![Star](https://img.shields.io/github/stars/OpenGVLab/GITM.svg?style=social&label=Star) <br> [**Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**](https://arxiv.org/abs/2305.17144) <br> |  Arxiv | 2023 | [Github](https://github.com/OpenGVLab/GITM) | - |
| ![Star](https://img.shields.io/github/stars/PKU-RL/Creative-Agents.svg?style=social&label=Star) <br> [**Creative Agents: Empowering Agents with Imagination for Creative Tasks**](https://arxiv.org/abs/2312.02519) <br> |  Arxiv | 2023 | [Github](https://github.com/PKU-RL/Creative-Agents) | [Demo](https://sites.google.com/view/creative-agents) |
|  [**Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**](https://arxiv.org/abs/2312.09238) <br> | CVPR | 2024 | - | [Demo](https://yangxue0827.github.io/auto_mc-reward.html) |
| ![Star](https://img.shields.io/github/stars/zju-vipa/Odyssey.svg?style=social&label=Star) <br> [**Odyssey: Empowering Minecraft Agents with Open-World Skills**](https://arxiv.org/abs/2407.15325) <br> |  Arxiv | 2024 | [Github](https://github.com/zju-vipa/Odyssey) | - |
| ![Star](https://img.shields.io/github/stars/rese1f/STEVE.svg?style=social&label=Star) <br> [**See and Think: Embodied Agent in Virtual Environment**](https://arxiv.org/abs/2311.15209) <br> |  ECCV | 2024 | [Github](https://github.com/rese1f/STEVE) | [Demo](https://rese1f.github.io/STEVE/) |
| [**RL-GPT: Integrating Reinforcement Learning and Code-as-policy**](https://arxiv.org/abs/2402.19299) <br> |  Arxiv | 2024 | - | [Demo](https://sites.google.com/view/rl-gpt) |
| [**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**](https://arxiv.org/abs/2405.17424) <br> |  Arxiv | 2024 | - | [Demo](https://lizhuoling.github.io/LARM_webpage/) |
| [**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**](https://arxiv.org/abs/2405.15414) <br> |  Arxiv | 2024 | - | - |


